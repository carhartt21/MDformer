TRAIN: 
  log_path: './weights' # path of weights
  use_cuda: True
  seed: 21
  start_epoch: 0 # if resume, set this as a number
  end_epoch: 200 # number of epochs of training
  #contnue here
  batch_size_per_gpu: 10 # input batch size
  num_workers: 20 # # threads for loading data
  lr_MN: 1e-6 # initial learning rate for the mapping network
  optim_beta: [0.5, 0.999] # momentum terms of adam
  lr_scheduler: true
  w_GAN: 2.0 # weight for GAN loss
  w_StyleRecon: 5.0 # weight for style recon loss
  w_StyleDiv: 0.0 # weight for style loss
  w_NCE: 1.0 # weight for NCE loss
  w_Instance_NCE: 0.0 # weight for instance NCE loss
  n_bbox: -1 # # box
  w_Cycle : 0.0
  
  save_epoch: 5 # frequency of saving checkpoints at the end of epochs

  distributed: True # if true use multiprocess distributed training
  gpu_ids: [0] # gpu ids: e.g. 0, 1, 2. use -1 for CPU
  epoch_iters: 8000 # number of iterations per epoch
  LR_SCHEDULER: 
    name: 'onecycle' # learning rate policy. see the details in pytorch
    # step_size: 100 # frequency of changing learning rate
    # gamma: 0.5 # new learning rate = gamma * old learning rate

MODEL:
  name: 'StarFormer' # name of the experiment. It decides where to store samples and models
  weight_path : './weights' # if training from scratch, set this as None
  feat_layers: [0,4,8] # compute NCE loss on which layers
  num_patches: 128 # number of patches per layer
  patch_size: 4  # patch size
  num_neg: 64 # number of negative samples
  num_pos: 1 # number of positive samples
  img_size: (256, 256) # scale images to this size
  fc_dim : 2048
  # Style code dimension
  style_dim : 64
  # Latent code dimension
  latent_dim : 16
  # dimension of the semantic embedding
  sem_embed_dim : 32
  # number of content channels
  content_dim : 128
  # number of downsampling layers in the content encoder
  n_downsampling : 2
  # number of input filters of the generator
  n_generator_filters : 64
  # number of discriminator filters
  n_discriminator_filters : 64
  # hidden dimension in the mapping network
  hidden_dim : 512
  VIT:
    # number of feature channels of the transformer
    feat_C : 256
    # number of transformer layers
    depth : 6
    # number of heads in each transformer layer
    heads : 8
    # dimension of each head
    mlp_dim : 4352

  SWIN:
    # number of input channels of the transformer
    embed_C : 1024

DATASET:
  target_domain_names: ['summer', 'winter', 'spring', 'autumn'] # domains to be used for training
  ref_dir: '/home/chge7185/datasets/StarFormer/ref_images' # path of reference images
  train_dir: '/home/chge7185/datasets/StarFormer/images/' # path of input images
  max_dataset_size: -1 # Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded. -1 : no maximum number of samples
  

VISUAL:
  visdom: True # visualize the training process using visdom
  display_sample_iter: 500 # frequency of displaying the latest results in visdom
  display_losses_iter: 100 # frequency of displaying losses in visdom  
  display_winsize: 256 # visdom display size
  image_save_iter: 500 # frequency of saving the latest training images
  print_losses_iter: 50 # frequency of showing training results on console  
  display_ncols:  4 # if positive, display all images in a single visdom web panel with certain number of images per row.
  display_id: 1 # window id of the web display.
  server: "http://localhost" # visdom server of the web display
  env:  'main' # visdom display environment name (default is "main")
  port: 8099 # visdom port of the web display
  save_results_freq: 500 # frequency of saving training results to html
  save_intermediate: True # do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/
